import os
import requests
import gradio as gr
import tiktoken
import re
from typing import List, Dict, Any

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –¥–ª—è LangChain —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
import chromadb
from sentence_transformers import SentenceTransformer
import numpy as np

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
models = [
    {
        "doc": "https://docs.google.com/document/d/–í–ê–®_ID/edit",
        "prompt": '''–¢—ã - –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –æ–Ω–ª–∞–π–Ω-—à–∫–æ–ª—ã, —Ç–≤–æ—è –∑–∞–¥–∞—á–∞ –±—ã—Å—Ç—Ä–æ, —Ç–æ—á–Ω–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã.
                        –ì–æ–≤–æ—Ä–∏ —Ç–æ–ª—å–∫–æ –ø–æ –¥–µ–ª—É, –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π —à—É—Ç–∫–∏, –Ω–µ –ø—Ä–æ—è–≤–ª—è–π —ç–º–æ—Ü–∏–π.
                        –û–±—â–∞–π—Å—è –≤–µ–∂–ª–∏–≤–æ, –Ω–æ —Å—Ç—Ä–æ–≥–æ. –ù–µ –≤—ã–¥–∞–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –±–∞–∑–µ.
                        –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –ø–æ —Ç–µ–º–µ ‚Äî –Ω–∞–ø—Ä–∞–≤—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É.
                        ''',
        "name": "–ù–µ–π—Ä–æ-–º–µ–Ω–µ–¥–∂–µ—Ä –æ–Ω–ª–∞–π–Ω-—à–∫–æ–ª—ã",
        "query": "–ö–∞–∫ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è?"
    },
]


class LocalGPT:
    """–ù–µ–π—Ä–æ-—Å–æ—Ç—Ä—É–¥–Ω–∏–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

    def __init__(self, model="llama2:7b-chat", ollama_url="http://127.0.0.1:11434/"):
        self.log = ''
        self.model = model
        self.ollama_url = ollama_url
        self.search_index = None
        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')  # –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.chroma_client = chromadb.Client()
        self.collection = None

    def load_search_indexes(self, url: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ Google Docs"""
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)
            if match_ is None:
                raise ValueError('–ù–µ–≤–µ—Ä–Ω—ã–π Google Docs URL')

            doc_id = match_.group(1)

            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')
            response.raise_for_status()

            text = response.text
            self.log += f'–î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω, —Ä–∞–∑–º–µ—Ä: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤\n'

            return self.create_embedding(text)

        except Exception as e:
            self.log += f'–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}\n'
            return None

    def create_embedding(self, text: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
        try:
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.split_text(text, chunk_size=1000, overlap=100)

            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ ChromaDB
            collection_name = f"documents_{len(text)}"
            try:
                self.chroma_client.delete_collection(collection_name)
            except:
                pass

            self.collection = self.chroma_client.create_collection(collection_name)

            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            embeddings = []
            documents = []
            ids = []

            for i, chunk in enumerate(chunks):
                if chunk.strip():  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —á–∞–Ω–∫ –Ω–µ –ø—É—Å—Ç–æ–π
                    embedding = self.embeddings_model.encode(chunk)
                    embeddings.append(embedding.tolist())
                    documents.append(chunk)
                    ids.append(f"doc_{i}")

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
            if documents:
                self.collection.add(
                    embeddings=embeddings,
                    documents=documents,
                    ids=ids
                )

                self.log += f'–°–æ–∑–¥–∞–Ω–æ {len(documents)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n'
                self.log += f'–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ\n'
                return True
            else:
                self.log += '–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞\n'
                return False

        except Exception as e:
            self.log += f'–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}\n'
            return False

    def split_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = []
        lines = text.split('\n')
        current_chunk = ""

        for line in lines:
            if len(current_chunk) + len(line) < chunk_size:
                current_chunk += line + '\n'
            else:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = line + '\n'

        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        return chunks

    def search_similar_docs(self, query: str, k: int = 3) -> List[str]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not self.collection:
            return []

        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embeddings_model.encode(query)

            # –ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=k
            )

            return results['documents'][0] if results['documents'] else []

        except Exception as e:
            self.log += f'–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}\n'
            return []

    def query_ollama(self, messages: List[Dict[str, str]], temperature: float = 0.7) -> str:
        """–ó–∞–ø—Ä–æ—Å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Ollama"""
        try:
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏
            prompt = ""
            for message in messages:
                role = message["role"]
                content = message["content"]

                if role == "system":
                    prompt += f"System: {content}\n\n"
                elif role == "user":
                    prompt += f"User: {content}\n\nAssistant: "

            # –ó–∞–ø—Ä–æ—Å –∫ Ollama
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                },
                timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                return result.get("response", "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏")
            else:
                self.log += f'–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama: {response.status_code}\n'
                return "–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –º–æ–¥–µ–ª–∏"

        except requests.exceptions.ConnectionError:
            return "–û—à–∏–±–∫–∞: Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ 'ollama serve' –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ"
        except Exception as e:
            self.log += f'–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏: {str(e)}\n'
            return f"–û—à–∏–±–∫–∞: {str(e)}"

    def answer_index(self, system_prompt: str, query: str, temperature: float = 0.7) -> str:
        """–û—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã"""
        if not self.collection:
            self.log += '–ú–æ–¥–µ–ª—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±—É—á–∏—Ç—å!\n'
            return '–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.'

        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        docs = self.search_similar_docs(query, k=3)

        if not docs:
            self.log += '–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n'
            return '–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.'

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = "\n\n".join([f"–§—Ä–∞–≥–º–µ–Ω—Ç {i + 1}:\n{doc}" for i, doc in enumerate(docs)])
        self.log += f'–ù–∞–π–¥–µ–Ω–æ {len(docs)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤\n'

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –º–æ–¥–µ–ª–∏
        messages = [
            {
                "role": "system",
                "content": system_prompt + f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n{context}"
            },
            {
                "role": "user",
                "content": query
            }
        ]

        # –ó–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏
        response = self.query_ollama(messages, temperature)

        return response


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è Gradio –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
def create_neural_employee(model_config):
    """–°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –Ω–µ–π—Ä–æ-—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞"""
    gpt = LocalGPT()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    success = gpt.load_search_indexes(model_config["doc"])

    if success:
        # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        response = gpt.answer_index(
            model_config["prompt"],
            model_config["query"]
        )

        return f"""
## {model_config["name"]}

**–°—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è:** ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω

**–¢–µ—Å—Ç–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å:** {model_config["query"]}

**–û—Ç–≤–µ—Ç:**
{response}
"""
    else:
        return f"""
## {model_config["name"]}

**–°—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è:** ‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è

**–õ–æ–≥–∏ –æ—à–∏–±–æ–∫:**
{gpt.log}"""


def chat_with_employee(message, model_config):
    """–ß–∞—Ç —Å –Ω–µ–π—Ä–æ-—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–º"""
    gpt = LocalGPT()
    gpt.load_search_indexes(model_config["doc"])

    response = gpt.answer_index(model_config["prompt"], message)

    return response


# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if __name__ == "__main__":
    demo = gr.Blocks(title="–ù–µ–π—Ä–æ-—Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –æ–Ω–ª–∞–π–Ω-—à–∫–æ–ª—ã")

    with demo:
        gr.Markdown("""
            # ü§ñ –ù–µ–π—Ä–æ-—Å–æ—Ç—Ä—É–¥–Ω–∏–∫ –æ–Ω–ª–∞–π–Ω-—à–∫–æ–ª—ã
            –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å! –ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∏ –∑–∞–¥–∞—Ç—å –µ–π –≤–æ–ø—Ä–æ—Å—ã.
            """)

        with gr.Tab("üìò –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"):
            gr.Markdown("### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏")
            gr.Markdown("–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –Ω–∏–∂–µ, —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ –æ–±—É—á–∏—Ç—å –Ω–µ–π—Ä–æ-—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞.")
            train_btn = gr.Button("üéì –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å", variant="primary")
            training_output = gr.Markdown("")

        with gr.Tab("üí¨ –ß–∞—Ç —Å –±–æ—Ç–æ–º"):
            gr.Markdown("### –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å")
            with gr.Row():
                chat_input = gr.Textbox(
                    placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ù–µ —É–¥–∞–µ—Ç—Å—è –≤–æ–π—Ç–∏ –≤ –∞–∫–∫–∞—É–Ω—Ç?",
                    label="–í–∞—à –≤–æ–ø—Ä–æ—Å",
                    lines=2
                )
                chat_btn = gr.Button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å", variant="primary")

            chat_output = gr.Textbox(
                label="–û—Ç–≤–µ—Ç –æ–Ω–ª–∞–π–Ω-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞",
                lines=8,
                max_lines=15,
                interactive=False
            )

            gr.Examples(
                examples=[
                    ["–ö–æ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –∫—É—Ä—Å—ã?"],
                    ["–ö–∞–∫ –æ–ø–ª–∞—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ?"]
                ],
                inputs=chat_input
            )

        train_btn.click(
            fn=lambda: create_neural_employee(models[0]),
            outputs=training_output
        )

        chat_btn.click(
            fn=lambda msg: chat_with_employee(msg, models[0]),
            inputs=chat_input,
            outputs=chat_output
        )

    demo.launch(
        server_name="localhost",
        server_port=7878,
        share=False
    )
